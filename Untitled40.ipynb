Q1. Ridge Regression is a regularization technique that adds a penalty term to the cost function to reduce overfitting, differing from OLS by shrinking coefficients towards zero.

Q2. Assumptions of Ridge Regression include linearity, independence, homoscedasticity, normality, and no multicollinearity, although it's more robust to multicollinearity than OLS.

Q3. The value of lambda is selected using cross-validation, grid search, or Bayesian optimization to find the optimal value that balances bias and variance.

Q4. Yes, Ridge Regression can be used for feature selection by analyzing the coefficients, where features with near-zero coefficients can be removed.

Q5. Ridge Regression performs well in the presence of multicollinearity by reducing the magnitude of correlated coefficients, making it a suitable choice for datasets with high multicollinearity.

Q6. Yes, Ridge Regression can handle both categorical and continuous independent variables, but categorical variables need to be encoded properly before modeling.

Q7. The coefficients of Ridge Regression represent the change in the dependent variable for a one-unit change in the independent variable, while controlling for other variables.

Q8. Yes, Ridge Regression can be used for time-series data analysis by incorporating temporal dependencies and using techniques like differencing and seasonal decomposition to make the data stationary.
